{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b6bc57c",
   "metadata": {},
   "source": [
    "# Guía paso a paso del notebook\n",
    "\n",
    "Este documento contiene implementaciones y comparaciones de clasificadores multiclase. A continuación se explica, por secciones (celdas), qué hace cada parte y qué esperar al ejecutar el notebook.\n",
    "\n",
    "1) Imports\n",
    "- `numpy`: operaciones y arrays.\n",
    "- `BaseEstimator`: base para crear estimadores estilo sklearn.\n",
    "- `LogisticRegression`: clasificador binario usado como base.\n",
    "- `OneVsRestClassifier`, `OneVsOneClassifier`: wrappers de sklearn para convertir clasificadores binarios en multiclase.\n",
    "- `combinations`: utilidad para enumerar pares de clases.\n",
    "\n",
    "Qué revisar: ejecutar esta celda primero para asegurar que todas las librerías están disponibles.\n",
    "\n",
    "2) Clase `OVAclasificador`\n",
    "- Propósito: resolver multiclase mediante One-vs-Rest delegando en `OneVsRestClassifier(LogisticRegression(...))`.\n",
    "- Métodos:\n",
    "  - `Fit(X, y)`: entrena el wrapper y guarda `self.clf` y `self.clases_` (etiquetas únicas).\n",
    "  - `Predict(X)`: devuelve etiquetas predichas (usa `self.clf.predict`).\n",
    "  - `Predict_proba(X)`: devuelve probabilidades por clase (si `predict_proba` disponible) o intenta `decision_function`.\n",
    "- Estados importantes: `self.clf`, `self.clases_`.\n",
    "\n",
    "3) Clase `OVOclasificador`\n",
    "- Propósito: One-vs-One delegando en `OneVsOneClassifier(LogisticRegression(...))`.\n",
    "- Métodos:\n",
    "  - `Fit(X, y)`: entrena `OneVsOneClassifier`, guarda `self.clf`, `self.clases_` y construye `self.modelos` mapeando pares de clases a estimadores binarios.\n",
    "  - `Predict(X)`: delega en `self.clf.predict`.\n",
    "- Notas: OVO crea K*(K-1)/2 clasificadores; puede ser más costoso para muchas clases.\n",
    "\n",
    "4) Clase `SoftmaxRegression`\n",
    "- Propósito: implementación propia de regresión logística multinomial (softmax) con descenso por gradiente.\n",
    "- Estructura:\n",
    "  - `self.W`: pesos shape (d+1, K) donde fila 0 = bias.\n",
    "  - `Fit(X, y)`: construye matriz one-hot, calcula softmax, gradiente de cross-entropy con L2 y actualiza `W` por iteraciones hasta convergencia ó `max_iter`.\n",
    "  - `Predict(X)`: calcula probabilidades por softmax y devuelve la clase con mayor prob.\n",
    "- Recomendación: para producción usar `LogisticRegression(multi_class='multinomial')` de sklearn; la implementación aquí es pedagógica.\n",
    "\n",
    "5) Celda de evaluación\n",
    "- Flujo:\n",
    "  1. Cargar dataset Iris (`X, y`).\n",
    "  2. `train_test_split(..., stratify=y)` para mantener proporciones por clase.\n",
    "  3. Escalar con `StandardScaler` ajustado en `X_train`.\n",
    "  4. Instanciar modelos (`ova`, `ovo`, `sm`) y para cada uno:\n",
    "     - `Fit(X_train_s, y_train)`\n",
    "     - `Predict(X_test_s)`\n",
    "     - medir tiempo y calcular `accuracy`, `classification_report` y `confusion_matrix`.\n",
    "  5. Mostrar resumen con accuracy y tiempo por modelo.\n",
    "\n",
    "6) Consejos y edge-cases\n",
    "- Ejecuta las celdas en orden (imports → definiciones → evaluación).\n",
    "- Llamar `Predict` antes de `Fit` lanzará `RuntimeError` intencional.\n",
    "- Si un estimador base no soporta `predict_proba`, el código intenta `decision_function` y aplica sigmoide; esas \"probabilidades\" pueden no estar calibradas.\n",
    "- Para muchos labels, OVO puede ser costoso; OVA o la solución multinomial suelen ser mejores.\n",
    "\n",
    "Si quieres, puedo también:\n",
    "- Añadir docstrings explicativos dentro de cada clase (reemplazando o ampliando los existentes), o\n",
    "- Ejecutar la celda de evaluación y pegar aquí los resultados (salida de consola). ¿Qué prefieres?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d857b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy: operaciones numéricas y arrays.\n",
    "# Uso: X = np.asarray(X) -> asegura tipo numpy.ndarray, shape (n_samples, n_features)\n",
    "import numpy as np\n",
    "\n",
    "# BaseEstimator: clase base para implementar estimadores con API similar a sklearn\n",
    "# Ejemplo: class MiClasificador(BaseEstimator): def Fit(self, X, y): ...\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "# LogisticRegression: clasificador binario\n",
    "# Parámetros principales: C (float, default=1.0), solver (str, ejemplo 'lbfgs'), max_iter (int)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Wrappers para convertir clasificadores binarios en multiclase\n",
    "# OneVsRestClassifier: entrena K clasificadores (uno por clase)\n",
    "# OneVsOneClassifier: entrena K*(K-1)/2 clasificadores (uno por cada par de clases)\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "\n",
    "# Utilidad para generar pares ordenados de clases\n",
    "from itertools import combinations\n",
    "\n",
    "# Nota: si necesitas conteos rápidos puedes importar Counter desde collections\n",
    "# from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7834743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-vs-All (One-vs-Rest) utilizando LogisticRegression como base\n",
    "class OVAclasificador(BaseEstimator):\n",
    "\n",
    "    def __init__(self, C=1.0, solver='lbfgs', max_iter=1000):\n",
    "        # C: float que controla regularización L2 inversa. Ejemplo: C=1.0 (por defecto).\n",
    "        # solver: optimizador interno de LogisticRegression. Ejemplo: 'lbfgs'.\n",
    "        # max_iter: iteraciones máximas del solver. Ejemplo: 1000.\n",
    "        self.C = C\n",
    "        self.solver = solver\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        # self.clf: después de Fit contendrá la instancia OneVsRestClassifier entrenada.\n",
    "        # Antes de Fit: None\n",
    "        # Después de Fit: OneVsRestClassifier con atributos .estimators_ y .classes_\n",
    "        self.clf = None\n",
    "\n",
    "        # self.clases_: array numpy 1D con las etiquetas únicas ordenadas.\n",
    "        # Antes de Fit: None\n",
    "        # Después de Fit: np.array([0,1,2]) por ejemplo para Iris\n",
    "        self.clases_ = None\n",
    "\n",
    "    def Fit(self, X, y):\n",
    "        # X: array convertible a numpy, shape (n_samples, n_features)\n",
    "        # y: array convertible a numpy, shape (n_samples,)\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        # Crear estimador base\n",
    "        base = LogisticRegression(C=self.C, solver=self.solver, max_iter=self.max_iter)\n",
    "\n",
    "        # OneVsRestClassifier encapsula el estimador base creando K clasificadores binarios\n",
    "        # Cada clasificador distingue una etiqueta vs resto\n",
    "        self.clf = OneVsRestClassifier(base)\n",
    "\n",
    "        # Entrenamiento: tras esto self.clf.estimators_ tiene K estimadores\n",
    "        self.clf.fit(X, y)\n",
    "\n",
    "        # Guardar el orden de clases; importante para interpretar predict_proba (columnas)\n",
    "        self.clases_ = getattr(self.clf, 'classes_', None)\n",
    "\n",
    "        # Retornar self para compatibilidad con estilo sklearn\n",
    "        return self\n",
    "\n",
    "    def Predict(self, X):\n",
    "        # X: shape (m, n_features) donde m=número de muestras a predecir\n",
    "        X = np.asarray(X)\n",
    "        if self.clf is None:\n",
    "            # Evitar uso antes de entrenar\n",
    "            raise RuntimeError(\"El clasificador no está entrenado. Ejecutar Fit primero.\")\n",
    "\n",
    "        # Devuelve array de etiquetas shape (m,)\n",
    "        return self.clf.predict(X)\n",
    "\n",
    "    def Predict_proba(self, X):\n",
    "        # Devuelve probabilidades en shape (m, n_classes)\n",
    "        X = np.asarray(X)\n",
    "        if self.clf is None:\n",
    "            raise RuntimeError(\"El clasificador no está entrenado. Ejecutar Fit primero.\")\n",
    "\n",
    "        # Si el wrapper soporta predict_proba devuelve directamente la matriz\n",
    "        if hasattr(self.clf, 'predict_proba'):\n",
    "            # Cada columna corresponde a la clase en self.clases_\n",
    "            return self.clf.predict_proba(X)\n",
    "\n",
    "        # Si predict_proba no está disponible, intentar con decision_function\n",
    "        if hasattr(self.clf, 'decision_function'):\n",
    "            df = self.clf.decision_function(X)\n",
    "            # df puede ser shape (m, n_classes) o (m,) para binario; aplicamos sigmoide\n",
    "            return 1 / (1 + np.exp(-df))\n",
    "\n",
    "        # Si ninguna opción está disponible, informar\n",
    "        raise AttributeError(\"El clasificador base no soporta predict_proba ni decision_function\")\n",
    "\n",
    "# Ejemplos de uso (no ejecutarlos aquí):\n",
    "# ova = OVAclasificador(C=0.5, solver='liblinear')\n",
    "# ova.Fit(X_train, y_train)\n",
    "# preds = ova.Predict(X_test)          # array shape (n_test,)\n",
    "# proba = ova.Predict_proba(X_test)    # array shape (n_test, n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19515dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-vs-One (OvO) wrapper delegando en sklearn OneVsOneClassifier\n",
    "class OVOclasificador(BaseEstimator):\n",
    "\n",
    "    def __init__(self, C=1.0, solver='lbfgs', max_iter=1000):\n",
    "        # Hiperparámetros para los clasificadores binarios\n",
    "        self.C = C\n",
    "        self.solver = solver\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        # self.modelos: diccionario {(clase_a, clase_b): estimator}\n",
    "        # Se llena en Fit a partir de self.clf.estimators_\n",
    "        self.modelos = {}\n",
    "\n",
    "        # self.clases_: np.array de etiquetas únicas; antes None, después np.unique(y)\n",
    "        self.clases_ = None\n",
    "\n",
    "        # self.clf: instancia OneVsOneClassifier después de Fit\n",
    "        self.clf = None\n",
    "\n",
    "    def Fit(self, X, y):\n",
    "        # X: (n_samples, n_features), y: (n_samples,)\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        # Construir y entrenar OneVsOneClassifier\n",
    "        base = LogisticRegression(C=self.C, solver=self.solver, max_iter=self.max_iter)\n",
    "        self.clf = OneVsOneClassifier(base)\n",
    "        self.clf.fit(X, y)\n",
    "\n",
    "        # Guardar clases en el orden que sklearn usa internamente\n",
    "        self.clases_ = getattr(self.clf, 'classes_', None)\n",
    "\n",
    "        # estimators_ lista: contiene K*(K-1)/2 estimadores\n",
    "        estimators = getattr(self.clf, 'estimators_', None)\n",
    "        if estimators is not None and self.clases_ is not None:\n",
    "            # Asociar cada estimator con el par de clases correspondiente\n",
    "            # combinations(self.clases_, 2) genera pares en el mismo orden esperado\n",
    "            for (a, b), est in zip(list(combinations(self.clases_, 2)), estimators):\n",
    "                self.modelos[(a, b)] = est\n",
    "        else:\n",
    "            self.modelos = {}\n",
    "\n",
    "        return self\n",
    "\n",
    "    def Predict(self, X):\n",
    "        # X: (m, n_features)\n",
    "        X = np.asarray(X)\n",
    "        if self.clf is None:\n",
    "            raise RuntimeError(\"El clasificador no está entrenado. Ejecutar Fit primero.\")\n",
    "\n",
    "        # OneVsOneClassifier realiza la votación internamente y devuelve etiquetas\n",
    "        return self.clf.predict(X)\n",
    "\n",
    "# Ejemplo: para K=3 (clases {0,1,2}) se crean 3 clasificadores: (0,1),(0,2),(1,2)\n",
    "# estimators_ tendrá 3 elementos; self.modelos contendrá claves (0,1),(0,2),(1,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c47ec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de regresión logística multinomial (softmax) con descenso por gradiente\n",
    "class SoftmaxRegression(BaseEstimator):\n",
    "    \"\"\"Softmax implementado manualmente.\n",
    "\n",
    "    Parámetros y ejemplos:\n",
    "    - C: (opcional) si se proporciona, reg = 1/C (float). Ejemplo: C=1.0 -> reg=1.0\n",
    "    - lr: learning rate (float). Recomendado: 0.01 - 1.0 según problema. Valor por defecto 0.1.\n",
    "    - max_iter: iteraciones máximas (int), ejemplo 1000 o 2000.\n",
    "    - tol: tolerancia para convergencia en norma de cambio de W, ejemplo 1e-5.\n",
    "    - reg: regularización L2 directa (si se prefiere pasar explicitamente). Si reg no es None usa ese valor.\n",
    "\n",
    "    Shapes importantes:\n",
    "    - X: (n, d)\n",
    "    - Xb (con bias): (n, d+1)\n",
    "    - W: (d+1, K) donde K = número de clases\n",
    "    - probs: (n, K)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, C=None, lr=0.1, max_iter=1000, tol=1e-5, reg=None, verbose=False, **kwargs):\n",
    "        # Determinar regularización: si reg no se pasa, tomar 1/C si C fue provisto\n",
    "        if reg is None:\n",
    "            if C is None:\n",
    "                # Valor por defecto pequeño para evitar overfitting sin C\n",
    "                self.reg = 1e-3\n",
    "            else:\n",
    "                self.reg = 1.0 / C\n",
    "        else:\n",
    "            self.reg = reg\n",
    "\n",
    "        # Learning rate: controla tamaño de paso en descenso por gradiente\n",
    "        self.lr = lr\n",
    "        # Iteraciones máximas\n",
    "        self.max_iter = max_iter\n",
    "        # Tolerancia de convergencia en norma de cambio de W\n",
    "        self.tol = tol\n",
    "        # Verbose para imprimir progreso\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Pesos W inicializados en Fit; antes None\n",
    "        self.W = None\n",
    "        # Clases detectadas en Fit; antes None\n",
    "        self.classes_ = None\n",
    "\n",
    "    def _one_hot(self, y_idx, K):\n",
    "        # y_idx: vector de índices en rango 0..K-1, shape (n,)\n",
    "        n = y_idx.shape[0]\n",
    "        Y = np.zeros((n, K), dtype=float)\n",
    "        Y[np.arange(n), y_idx] = 1.0\n",
    "        return Y\n",
    "\n",
    "    def Fit(self, X, y):\n",
    "        # Asegurar arrays numpy\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        # Detectar clases únicas y mapear a índices 0..K-1\n",
    "        self.classes_ = np.unique(y)\n",
    "        K = len(self.classes_)\n",
    "        class_to_idx = {c: i for i, c in enumerate(self.classes_)}\n",
    "        # y_idx contiene índices 0..K-1 correspondientemente\n",
    "        y_idx = np.vectorize(class_to_idx.get)(y)\n",
    "\n",
    "        # Añadir columna de unos para bias: Xb shape (n, d+1)\n",
    "        n, d = X.shape\n",
    "        Xb = np.hstack([np.ones((n, 1)), X])\n",
    "\n",
    "        # Inicializar W: ceros (d+1, K). Alternativa: pequeña aleatoriedad ayuda a romper simetrías\n",
    "        self.W = np.zeros((d + 1, K), dtype=float)\n",
    "\n",
    "        # Matriz one-hot de etiquetas Y shape (n, K)\n",
    "        Y = self._one_hot(y_idx, K)\n",
    "\n",
    "        # Descenso por gradiente\n",
    "        for it in range(self.max_iter):\n",
    "            # logits: Xb.dot(W) -> shape (n, K)\n",
    "            scores = Xb.dot(self.W)\n",
    "            # Estabilizar antes de softmax\n",
    "            scores -= scores.max(axis=1, keepdims=True)\n",
    "            exp_scores = np.exp(scores)\n",
    "            probs = exp_scores / exp_scores.sum(axis=1, keepdims=True)  # (n, K)\n",
    "\n",
    "            # Gradiente de la función de pérdida (cross-entropy + L2)\n",
    "            grad = - (Xb.T.dot(Y - probs)) / n  # shape (d+1, K)\n",
    "            # Regularización L2 aplicada solo a pesos (no bias): primera fila de W no penalizada\n",
    "            reg_term = self.reg * np.vstack([np.zeros((1, K)), self.W[1:, :]])\n",
    "            grad += reg_term\n",
    "\n",
    "            # Actualizar W\n",
    "            W_old = self.W.copy()\n",
    "            self.W -= self.lr * grad\n",
    "\n",
    "            # Norm of change: criterio de parada\n",
    "            diff = np.linalg.norm(self.W - W_old)\n",
    "            if self.verbose and (it % 100 == 0 or it == self.max_iter - 1):\n",
    "                # Loss aproximado: cross-entropy media + L2 (sin bias)\n",
    "                loss = -np.mean(np.sum(Y * np.log(probs + 1e-15), axis=1)) + 0.5 * self.reg * np.sum(self.W[1:, :] ** 2)\n",
    "                print(f\"it={it} loss={loss:.6f} ||dW||={diff:.6e}\")\n",
    "            if diff < self.tol:\n",
    "                # Convergencia alcanzada\n",
    "                break\n",
    "\n",
    "        return self\n",
    "\n",
    "    def Predict(self, X):\n",
    "        # Comprobar que W fue entrenada\n",
    "        if self.W is None:\n",
    "            raise RuntimeError(\"El clasificador no está entrenado. Ejecutar Fit primero.\")\n",
    "        X = np.asarray(X)\n",
    "        n = X.shape[0]\n",
    "        Xb = np.hstack([np.ones((n, 1)), X])\n",
    "\n",
    "        # Calcular probabilidades y devolver etiqueta con mayor probabilidad\n",
    "        scores = Xb.dot(self.W)\n",
    "        scores -= scores.max(axis=1, keepdims=True)\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / exp_scores.sum(axis=1, keepdims=True)  # (n, K)\n",
    "        idx = np.argmax(probs, axis=1)\n",
    "        return self.classes_[idx]\n",
    "\n",
    "# Ejemplo de parámetros recomendados:\n",
    "# - lr=0.1, reg=1e-3 para datasets pequeños.\n",
    "# - Aumentar max_iter a 2000 si no converge.\n",
    "# - Usar verbose=True para ver progreso cada 100 it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2a1efda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OvA: accuracy=0.8444 time=0.050s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        15\n",
      "  versicolor       0.79      0.73      0.76        15\n",
      "   virginica       0.75      0.80      0.77        15\n",
      "\n",
      "    accuracy                           0.84        45\n",
      "   macro avg       0.85      0.84      0.84        45\n",
      "weighted avg       0.85      0.84      0.84        45\n",
      "\n",
      "Confusion matrix:\n",
      " [[15  0  0]\n",
      " [ 0 11  4]\n",
      " [ 0  3 12]]\n",
      "------------------------------------------------------------\n",
      "OvO: accuracy=0.9111 time=0.008s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        15\n",
      "  versicolor       0.82      0.93      0.88        15\n",
      "   virginica       0.92      0.80      0.86        15\n",
      "\n",
      "    accuracy                           0.91        45\n",
      "   macro avg       0.92      0.91      0.91        45\n",
      "weighted avg       0.92      0.91      0.91        45\n",
      "\n",
      "Confusion matrix:\n",
      " [[15  0  0]\n",
      " [ 0 14  1]\n",
      " [ 0  3 12]]\n",
      "------------------------------------------------------------\n",
      "Softmax: accuracy=0.9111 time=0.078s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        15\n",
      "  versicolor       0.82      0.93      0.88        15\n",
      "   virginica       0.92      0.80      0.86        15\n",
      "\n",
      "    accuracy                           0.91        45\n",
      "   macro avg       0.92      0.91      0.91        45\n",
      "weighted avg       0.92      0.91      0.91        45\n",
      "\n",
      "Confusion matrix:\n",
      " [[15  0  0]\n",
      " [ 0 14  1]\n",
      " [ 0  3 12]]\n",
      "------------------------------------------------------------\n",
      "Resumen:\n",
      "OvA: acc=0.8444 time=0.050s\n",
      "OvO: acc=0.9111 time=0.008s\n",
      "Softmax: acc=0.9111 time=0.078s\n"
     ]
    }
   ],
   "source": [
    "# Evaluación comparativa en el dataset Iris con explicaciones detalladas\n",
    "# Carga del dataset: 'data' contiene X (data.data) y y (data.target)\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import time\n",
    "\n",
    "# data: Bunch con campos .data (X) y .target (y)\n",
    "data = load_iris()\n",
    "# X: array (150, 4), y: array (150,) con etiquetas {0,1,2}\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Separar entrenamiento/prueba manteniendo proporciones de clase con 'stratify'\n",
    "# X_train: (105,4), X_test: (45,4) cuando test_size=0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Escalado: ajustar scaler en X_train y aplicar al conjunto de prueba\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)  # media ~0, desviación ~1 (por columna)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "# Instancias de los modelos. Estos objetos estarán sin entrenar hasta llamar a Fit.\n",
    "ova = OVAclasificador()\n",
    "ovo = OVOclasificador()\n",
    "try:\n",
    "    # intentar con firma de la implementación propia (lr, reg, max_iter)\n",
    "    sm = SoftmaxRegression(lr=0.5, max_iter=2000, tol=1e-7, verbose=False, reg=1e-3)\n",
    "except TypeError:\n",
    "    # fallback por compatibilidad\n",
    "    sm = SoftmaxRegression(C=1.0, solver='lbfgs', max_iter=2000)\n",
    "\n",
    "# Lista de tuplas (nombre, objeto) para iterar y comparar\n",
    "models = [('OvA', ova), ('OvO', ovo), ('Softmax', sm)]\n",
    "\n",
    "# results: dict donde guardamos info por modelo\n",
    "# keys: nombre modelo\n",
    "# values: dict con 'accuracy' (float), 'time_s' (float), 'y_pred' (array predicciones)\n",
    "results = {}\n",
    "\n",
    "for name, model in models:\n",
    "    # t0 guarda el tiempo antes de entrenar y predecir\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Entrenar el modelo con X_train_s (forma (n_train, n_features))\n",
    "    # Después de Fit, cada objeto tendrá internamente coeficientes/estados entrenados\n",
    "    model.Fit(X_train_s, y_train)\n",
    "\n",
    "    # Predict sobre X_test_s -> devuelve array (n_test,) con etiquetas predichas\n",
    "    y_pred = model.Predict(X_test_s)\n",
    "\n",
    "    # t1 guarda el tiempo después de entrenamiento+predicción\n",
    "    t1 = time.time()\n",
    "\n",
    "    # accuracy: número de aciertos / n_test\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Guardar resultados para posterior inspección\n",
    "    results[name] = {'accuracy': acc, 'time_s': t1 - t0, 'y_pred': y_pred}\n",
    "\n",
    "    # Imprimir métricas detalladas para este modelo\n",
    "    print(f\"{name}: accuracy={acc:.4f} time={t1-t0:.3f}s\")\n",
    "    # classification_report muestra precision/recall/f1 por clase (usa labels originales)\n",
    "    print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
    "    # confusion_matrix: matriz (n_clases, n_clases) donde rows=true, cols=pred\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Resumen compacto: mostrar accuracy y tiempos guardados en results\n",
    "print(\"Resumen:\")\n",
    "for name, info in results.items():\n",
    "    print(f\"{name}: acc={info['accuracy']:.4f} time={info['time_s']:.3f}s\")\n",
    "\n",
    "# Notas de interpretación:\n",
    "# - Si un modelo tiene baja precisión para una clase, inspeccionar su confusion matrix\n",
    "# - Softmax (implementación propia) puede requerir ajuste de lr y reg para converger bien\n",
    "# - Los tiempos reportados incluyen tanto entrenamiento como predicción\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
