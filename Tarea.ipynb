{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b6bc57c",
   "metadata": {},
   "source": [
    "# Guía paso a paso del notebook\n",
    "\n",
    "Este documento contiene implementaciones y comparaciones de clasificadores multiclase. A continuación se explica, por secciones (celdas), qué hace cada parte y qué esperar al ejecutar el notebook.\n",
    "\n",
    "1) Imports\n",
    "- `numpy`: operaciones y arrays.\n",
    "- `BaseEstimator`: base para crear estimadores estilo sklearn.\n",
    "- `LogisticRegression`: clasificador binario usado como base.\n",
    "- `OneVsRestClassifier`, `OneVsOneClassifier`: wrappers de sklearn para convertir clasificadores binarios en multiclase.\n",
    "- `combinations`: utilidad para enumerar pares de clases.\n",
    "\n",
    "Qué revisar: ejecutar esta celda primero para asegurar que todas las librerías están disponibles.\n",
    "\n",
    "2) Clase `OVAclasificador`\n",
    "- Propósito: resolver multiclase mediante One-vs-Rest delegando en `OneVsRestClassifier(LogisticRegression(...))`.\n",
    "- Métodos:\n",
    "  - `Fit(X, y)`: entrena el wrapper y guarda `self.clf` y `self.clases_` (etiquetas únicas).\n",
    "  - `Predict(X)`: devuelve etiquetas predichas (usa `self.clf.predict`).\n",
    "  - `Predict_proba(X)`: devuelve probabilidades por clase (si `predict_proba` disponible) o intenta `decision_function`.\n",
    "- Estados importantes: `self.clf`, `self.clases_`.\n",
    "\n",
    "3) Clase `OVOclasificador`\n",
    "- Propósito: One-vs-One delegando en `OneVsOneClassifier(LogisticRegression(...))`.\n",
    "- Métodos:\n",
    "  - `Fit(X, y)`: entrena `OneVsOneClassifier`, guarda `self.clf`, `self.clases_` y construye `self.modelos` mapeando pares de clases a estimadores binarios.\n",
    "  - `Predict(X)`: delega en `self.clf.predict`.\n",
    "- Notas: OVO crea K*(K-1)/2 clasificadores; puede ser más costoso para muchas clases.\n",
    "\n",
    "4) Clase `SoftmaxRegression`\n",
    "- Propósito: implementación propia de regresión logística multinomial (softmax) con descenso por gradiente.\n",
    "- Estructura:\n",
    "  - `self.W`: pesos shape (d+1, K) donde fila 0 = bias.\n",
    "  - `Fit(X, y)`: construye matriz one-hot, calcula softmax, gradiente de cross-entropy con L2 y actualiza `W` por iteraciones hasta convergencia ó `max_iter`.\n",
    "  - `Predict(X)`: calcula probabilidades por softmax y devuelve la clase con mayor prob.\n",
    "- Recomendación: para producción usar `LogisticRegression(multi_class='multinomial')` de sklearn; la implementación aquí es pedagógica.\n",
    "\n",
    "5) Celda de evaluación\n",
    "- Flujo:\n",
    "  1. Cargar dataset Iris (`X, y`).\n",
    "  2. `train_test_split(..., stratify=y)` para mantener proporciones por clase.\n",
    "  3. Escalar con `StandardScaler` ajustado en `X_train`.\n",
    "  4. Instanciar modelos (`ova`, `ovo`, `sm`) y para cada uno:\n",
    "     - `Fit(X_train_s, y_train)`\n",
    "     - `Predict(X_test_s)`\n",
    "     - medir tiempo y calcular `accuracy`, `classification_report` y `confusion_matrix`.\n",
    "  5. Mostrar resumen con accuracy y tiempo por modelo.\n",
    "\n",
    "6) Consejos y edge-cases\n",
    "- Ejecuta las celdas en orden (imports → definiciones → evaluación).\n",
    "- Llamar `Predict` antes de `Fit` lanzará `RuntimeError` intencional.\n",
    "- Si un estimador base no soporta `predict_proba`, el código intenta `decision_function` y aplica sigmoide; esas \"probabilidades\" pueden no estar calibradas.\n",
    "- Para muchos labels, OVO puede ser costoso; OVA o la solución multinomial suelen ser mejores.\n",
    "\n",
    "Si quieres, puedo también:\n",
    "- Añadir docstrings explicativos dentro de cada clase (reemplazando o ampliando los existentes), o\n",
    "- Ejecutar la celda de evaluación y pegar aquí los resultados (salida de consola). ¿Qué prefieres?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d857b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy: operaciones numéricas y arrays.\n",
    "# Uso: X = np.asarray(X) -> asegura tipo numpy.ndarray, shape (n_samples, n_features)\n",
    "import numpy as np\n",
    "\n",
    "# BaseEstimator: clase base para implementar estimadores con API similar a sklearn\n",
    "# Ejemplo: class MiClasificador(BaseEstimator): def Fit(self, X, y): ...\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "# LogisticRegression: clasificador binario\n",
    "# Parámetros principales: C (float, default=1.0), solver (str, ejemplo 'lbfgs'), max_iter (int)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Wrappers para convertir clasificadores binarios en multiclase\n",
    "# OneVsRestClassifier: entrena K clasificadores (uno por clase)\n",
    "# OneVsOneClassifier: entrena K*(K-1)/2 clasificadores (uno por cada par de clases)\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "\n",
    "# Utilidad para generar pares ordenados de clases\n",
    "from itertools import combinations\n",
    "\n",
    "# Nota: si necesitas conteos rápidos puedes importar Counter desde collections\n",
    "# from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dc5d1e",
   "metadata": {},
   "source": [
    "# Explicación Detallada: One-vs-All (OVA)\n",
    "\n",
    "## ¿Qué es One-vs-All?\n",
    "\n",
    "**One-vs-All (OVA)** o **One-vs-Rest** es una estrategia para resolver problemas de clasificación multiclase utilizando clasificadores binarios.\n",
    "\n",
    "### Concepto Principal:\n",
    "Para un problema con **K clases**, OVA entrena **K clasificadores binarios**:\n",
    "\n",
    "1. **Clasificador 1**: Clase 0 vs {Clase 1, Clase 2, ..., Clase K-1}\n",
    "2. **Clasificador 2**: Clase 1 vs {Clase 0, Clase 2, ..., Clase K-1}\n",
    "3. **...**\n",
    "4. **Clasificador K**: Clase K-1 vs {todas las demás clases}\n",
    "\n",
    "### Ejemplo con Dataset Iris (3 clases):\n",
    "- **Clasificador \"Setosa\"**: ¿Es Setosa? (Sí/No) → Setosa vs {Versicolor + Virginica}\n",
    "- **Clasificador \"Versicolor\"**: ¿Es Versicolor? (Sí/No) → Versicolor vs {Setosa + Virginica}\n",
    "- **Clasificador \"Virginica\"**: ¿Es Virginica? (Sí/No) → Virginica vs {Setosa + Versicolor}\n",
    "\n",
    "### Proceso de Predicción:\n",
    "1. Cada uno de los K clasificadores produce un **score/confianza** para su clase\n",
    "2. Se selecciona la clase con el **score más alto**\n",
    "3. Alternativamente, se pueden obtener **probabilidades calibradas** usando `predict_proba`\n",
    "\n",
    "### Ventajas ✅:\n",
    "- **Simple**: Fácil de entender e implementar\n",
    "- **Eficiente**: Solo K clasificadores (vs K×(K-1)/2 en OvO)\n",
    "- **Escalable**: Funciona bien con muchas clases\n",
    "- **Paralelizable**: Los K clasificadores se pueden entrenar independientemente\n",
    "\n",
    "### Desventajas ❌:\n",
    "- **Desbalance de clases**: Cada clasificador ve 1 clase positiva vs (K-1) clases negativas\n",
    "- **Calibración**: Las probabilidades pueden no estar bien calibradas entre clasificadores\n",
    "- **Solapamiento**: Problemas si las clases se solapan significativamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7834743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# IMPLEMENTACIÓN DE ONE-VS-ALL (OVA) USANDO LogisticRegression COMO BASE\n",
    "# ==================================================================================\n",
    "\n",
    "class OVAclasificador(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Clasificador multiclase usando estrategia One-vs-All (One-vs-Rest).\n",
    "    \n",
    "    PRINCIPIO DE FUNCIONAMIENTO:\n",
    "    - Para K clases, entrena K clasificadores binarios\n",
    "    - Cada clasificador distingue: \"una clase específica\" vs \"todas las demás\"\n",
    "    - En predicción: ejecuta los K clasificadores y elige la clase con mayor confianza\n",
    "    \n",
    "    EJEMPLO CON IRIS (3 clases):\n",
    "    - Clasificador 1: Setosa vs {Versicolor + Virginica}\n",
    "    - Clasificador 2: Versicolor vs {Setosa + Virginica}  \n",
    "    - Clasificador 3: Virginica vs {Setosa + Versicolor}\n",
    "    \n",
    "    DELEGACIÓN A SKLEARN:\n",
    "    Esta implementación delega en OneVsRestClassifier de sklearn para simplificar el código\n",
    "    y aprovechar optimizaciones ya probadas.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, C=1.0, solver='lbfgs', max_iter=1000):\n",
    "        \"\"\"\n",
    "        Inicializa el clasificador OVA.\n",
    "        \n",
    "        PARÁMETROS:\n",
    "        -----------\n",
    "        C : float, default=1.0\n",
    "            Parámetro de regularización INVERSA para LogisticRegression.\n",
    "            - C grande = menos regularización (puede hacer overfitting)\n",
    "            - C pequeño = más regularización (puede hacer underfitting)\n",
    "            \n",
    "        solver : str, default='lbfgs'\n",
    "            Algoritmo de optimización para LogisticRegression:\n",
    "            - 'lbfgs': Bueno para datasets pequeños, soporta regularización L2\n",
    "            - 'liblinear': Bueno para datasets grandes, soporta L1 y L2\n",
    "            - 'saga': Soporta L1, L2 y elastic net, bueno para datasets grandes\n",
    "            \n",
    "        max_iter : int, default=1000\n",
    "            Número máximo de iteraciones para que el solver converja.\n",
    "            Aumentar si aparecen warnings de convergencia.\n",
    "        \"\"\"\n",
    "        # Guardar hiperparámetros para los clasificadores binarios internos\n",
    "        self.C = C\n",
    "        self.solver = solver\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        # ESTADOS INTERNOS (se llenan durante Fit):\n",
    "        # ==========================================\n",
    "        \n",
    "        # self.clf: Instancia de OneVsRestClassifier después del entrenamiento\n",
    "        # Antes de Fit: None\n",
    "        # Después de Fit: OneVsRestClassifier con K estimadores en .estimators_\n",
    "        self.clf = None\n",
    "        \n",
    "        # self.clases_: Array numpy 1D con las etiquetas únicas ordenadas\n",
    "        # Antes de Fit: None  \n",
    "        # Después de Fit: np.array([0, 1, 2]) para Iris, por ejemplo\n",
    "        # IMPORTANTE: El orden determina cómo interpretar las columnas de predict_proba\n",
    "        self.clases_ = None\n",
    "\n",
    "    def Fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Entrena el clasificador OVA con los datos proporcionados.\n",
    "        \n",
    "        PROCESO INTERNO:\n",
    "        ================\n",
    "        1. Crear LogisticRegression base con los parámetros especificados\n",
    "        2. Envolver en OneVsRestClassifier (esto crea K copias del clasificador base)\n",
    "        3. OneVsRestClassifier internamente:\n",
    "           - Para cada clase i en {0, 1, ..., K-1}:\n",
    "             * Crea etiquetas binarias: y_binary = (y == i)\n",
    "             * Entrena clasificador_i con (X, y_binary)\n",
    "           - Guarda los K clasificadores en .estimators_\n",
    "        4. Guardar información sobre las clases para predicciones futuras\n",
    "        \n",
    "        PARÁMETROS:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Datos de entrenamiento. Cada fila es una muestra, cada columna una característica.\n",
    "            \n",
    "        y : array-like, shape (n_samples,)\n",
    "            Etiquetas de clase para entrenamiento.\n",
    "            Pueden ser números (0, 1, 2) o strings ('setosa', 'versicolor', etc.)\n",
    "            \n",
    "        RETORNA:\n",
    "        --------\n",
    "        self : OVAclasificador\n",
    "            El objeto entrenado (para compatibilidad con sklearn)\n",
    "        \"\"\"\n",
    "        # Convertir a arrays numpy para consistencia\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        # PASO 1: Crear el clasificador binario base\n",
    "        # ==========================================\n",
    "        # Este será el \"template\" que se copiará K veces\n",
    "        base = LogisticRegression(\n",
    "            C=self.C,                # Regularización inversa\n",
    "            solver=self.solver,      # Algoritmo de optimización  \n",
    "            max_iter=self.max_iter   # Límite de iteraciones\n",
    "        )\n",
    "        \n",
    "        # PASO 2: Crear el wrapper One-vs-Rest\n",
    "        # =====================================\n",
    "        # OneVsRestClassifier toma el clasificador base y automáticamente:\n",
    "        # - Detecta las K clases únicas en y\n",
    "        # - Crea K copias del clasificador base\n",
    "        # - Para cada clase i, entrena una copia con etiquetas binarias (clase_i vs resto)\n",
    "        self.clf = OneVsRestClassifier(base)\n",
    "        \n",
    "        # PASO 3: Entrenamiento efectivo\n",
    "        # ===============================\n",
    "        # Internamente esto ejecuta el bucle:\n",
    "        # for i, clase in enumerate(clases_unicas):\n",
    "        #     y_binario = (y == clase).astype(int)  # 1 si es clase_i, 0 si no\n",
    "        #     estimator_i = clone(base)\n",
    "        #     estimator_i.fit(X, y_binario)\n",
    "        #     estimators_.append(estimator_i)\n",
    "        self.clf.fit(X, y)\n",
    "        \n",
    "        # PASO 4: Guardar metadatos importantes\n",
    "        # ======================================\n",
    "        # Las clases detectadas, en el orden que sklearn las procesa\n",
    "        # Esto es crucial para interpretar las salidas de predict_proba correctamente\n",
    "        self.clases_ = getattr(self.clf, 'classes_', None)\n",
    "        \n",
    "        # ESTADO POST-ENTRENAMIENTO:\n",
    "        # self.clf.estimators_ contiene [clasificador_0, clasificador_1, ..., clasificador_K-1]\n",
    "        # self.clf.classes_ contiene [clase_0, clase_1, ..., clase_K-1] \n",
    "        # Cada clasificador_i distingue clase_i vs todas_las_demás\n",
    "        \n",
    "        return self  # Para permitir method chaining: ova.Fit(X, y).Predict(X_test)\n",
    "\n",
    "    def Predict(self, X):\n",
    "        \"\"\"\n",
    "        Predice las clases para nuevas muestras.\n",
    "        \n",
    "        PROCESO INTERNO:\n",
    "        ================\n",
    "        1. Verificar que el modelo fue entrenado\n",
    "        2. Para cada muestra en X:\n",
    "           - Ejecutar los K clasificadores binarios\n",
    "           - Cada uno produce un \"score\" o \"confianza\"\n",
    "           - Asignar la clase con mayor score\n",
    "           \n",
    "        DETALLES TÉCNICOS:\n",
    "        ==================\n",
    "        - self.clf.predict() internamente usa decision_function() o predict_proba()\n",
    "        - Si usa decision_function: score = w·x + b (sin transformar)\n",
    "        - Si usa predict_proba: score = probabilidad estimada\n",
    "        - Criterio de decisión: argmax(scores) sobre las K clases\n",
    "        \n",
    "        PARÁMETROS:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Muestras a clasificar. Debe tener las mismas características que en Fit.\n",
    "            \n",
    "        RETORNA:\n",
    "        --------\n",
    "        y_pred : array, shape (n_samples,)\n",
    "            Etiquetas predichas. Tienen el mismo tipo que las originales en Fit.\n",
    "            \n",
    "        EJEMPLO:\n",
    "        --------\n",
    "        Si X tiene 2 muestras y 3 clases [setosa, versicolor, virginica]:\n",
    "        - Muestra 1: scores = [0.8, 0.1, 0.1] → predicción: setosa\n",
    "        - Muestra 2: scores = [0.2, 0.7, 0.1] → predicción: versicolor\n",
    "        \"\"\"\n",
    "        # Convertir a array numpy\n",
    "        X = np.asarray(X)\n",
    "        \n",
    "        # VERIFICACIÓN DE ESTADO: ¿El modelo fue entrenado?\n",
    "        if self.clf is None:\n",
    "            raise RuntimeError(\n",
    "                \"❌ ERROR: El clasificador no está entrenado. \"\n",
    "                \"Debe ejecutar .Fit(X_train, y_train) antes de .Predict(X_test)\"\n",
    "            )\n",
    "        \n",
    "        # PREDICCIÓN DELEGADA:\n",
    "        # OneVsRestClassifier.predict() maneja internamente:\n",
    "        # 1. Ejecutar todos los K clasificadores binarios\n",
    "        # 2. Recopilar scores/probabilidades  \n",
    "        # 3. Aplicar argmax para cada muestra\n",
    "        # 4. Mapear índices de vuelta a etiquetas originales\n",
    "        return self.clf.predict(X)\n",
    "\n",
    "    def Predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Estima probabilidades de pertenencia a cada clase.\n",
    "        \n",
    "        INTERPRETACIÓN:\n",
    "        ===============\n",
    "        - Matriz de salida: shape (n_samples, n_classes)\n",
    "        - Fila i: probabilidades para la muestra i\n",
    "        - Columna j: probabilidad de pertenecer a self.clases_[j]\n",
    "        - Cada fila suma aproximadamente 1.0 (dependiendo de la calibración)\n",
    "        \n",
    "        CALIBRACIÓN:\n",
    "        ============\n",
    "        ⚠️  IMPORTANTE: En OVA las probabilidades pueden no estar perfectamente calibradas\n",
    "        porque cada clasificador binario se entrena independientemente.\n",
    "        Para una calibración mejor, considerar CalibratedClassifierCV de sklearn.\n",
    "        \n",
    "        MANEJO DE CASOS ESPECIALES:\n",
    "        ============================\n",
    "        - Si predict_proba no disponible: intenta decision_function + sigmoid\n",
    "        - Si ninguno disponible: lanza AttributeError\n",
    "        \n",
    "        PARÁMETROS:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Muestras para estimar probabilidades.\n",
    "            \n",
    "        RETORNA:\n",
    "        --------\n",
    "        probas : array, shape (n_samples, n_classes)\n",
    "            Probabilidades estimadas. probas[i, j] = P(muestra_i pertenece a clase_j)\n",
    "            \n",
    "        EJEMPLO:\n",
    "        --------\n",
    "        Para una muestra con 3 clases:\n",
    "        [[0.7, 0.2, 0.1],    # Muestra 1: 70% setosa, 20% versicolor, 10% virginica\n",
    "         [0.1, 0.8, 0.1]]    # Muestra 2: 10% setosa, 80% versicolor, 10% virginica\n",
    "        \"\"\"\n",
    "        # Convertir entrada\n",
    "        X = np.asarray(X)\n",
    "        \n",
    "        # Verificar entrenamiento\n",
    "        if self.clf is None:\n",
    "            raise RuntimeError(\n",
    "                \"❌ ERROR: El clasificador no está entrenado. \"\n",
    "                \"Ejecutar .Fit(X_train, y_train) primero.\"\n",
    "            )\n",
    "        \n",
    "        # CASO 1: predict_proba disponible (caso ideal)\n",
    "        if hasattr(self.clf, 'predict_proba'):\n",
    "            # OneVsRestClassifier.predict_proba() internamente:\n",
    "            # 1. Para cada clasificador binario, obtiene P(clase_i | x)\n",
    "            # 2. Normaliza para que las probabilidades sumen ~1\n",
    "            # 3. Devuelve matriz (n_samples, n_classes)\n",
    "            return self.clf.predict_proba(X)\n",
    "        \n",
    "        # CASO 2: Solo decision_function disponible (fallback)\n",
    "        elif hasattr(self.clf, 'decision_function'):\n",
    "            # decision_function devuelve scores sin calibrar\n",
    "            df = self.clf.decision_function(X)\n",
    "            \n",
    "            # Aplicar función sigmoid para convertir a \"pseudo-probabilidades\"\n",
    "            # sigmoid(x) = 1 / (1 + exp(-x))\n",
    "            # ⚠️ Estas NO son probabilidades verdaderas, solo aproximaciones\n",
    "            return 1 / (1 + np.exp(-df))\n",
    "        \n",
    "        # CASO 3: Ninguna opción disponible (error)\n",
    "        else:\n",
    "            raise AttributeError(\n",
    "                \"❌ ERROR: El clasificador base no soporta predict_proba ni decision_function. \"\n",
    "                \"Verificar la configuración del LogisticRegression base.\"\n",
    "            )\n",
    "\n",
    "# ==================================================================================\n",
    "# EJEMPLOS DE USO (comentados para no ejecutar automáticamente)\n",
    "# ==================================================================================\n",
    "\n",
    "# # EJEMPLO 1: Uso básico\n",
    "# ova = OVAclasificador(C=0.5, solver='liblinear', max_iter=2000)\n",
    "# ova.Fit(X_train, y_train)\n",
    "# \n",
    "# # Predicciones\n",
    "# y_pred = ova.Predict(X_test)          # Shape: (n_test,)\n",
    "# probas = ova.Predict_proba(X_test)    # Shape: (n_test, n_classes)\n",
    "# \n",
    "# # EJEMPLO 2: Inspeccionar clasificadores internos después del entrenamiento\n",
    "# print(f\"Número de clases detectadas: {len(ova.clases_)}\")\n",
    "# print(f\"Clases: {ova.clases_}\")\n",
    "# print(f\"Número de clasificadores binarios: {len(ova.clf.estimators_)}\")\n",
    "# \n",
    "# # EJEMPLO 3: Ver coeficientes de un clasificador específico (para interpretabilidad)\n",
    "# # ova.clf.estimators_[0] = clasificador que distingue clase_0 vs resto\n",
    "# # ova.clf.estimators_[1] = clasificador que distingue clase_1 vs resto\n",
    "# coef_clase_0 = ova.clf.estimators_[0].coef_\n",
    "# print(f\"Coeficientes para clase {ova.clases_[0]}: {coef_clase_0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004be6c1",
   "metadata": {},
   "source": [
    "# Ejemplo Paso a Paso: Cómo Funciona OVA Internamente\n",
    "\n",
    "## Simulación Manual del Proceso OVA\n",
    "\n",
    "Imaginemos que tenemos 6 muestras de Iris con estas características simplificadas:\n",
    "\n",
    "```\n",
    "X_train = [[5.1, 3.5],    # Muestra 1\n",
    "           [4.9, 3.0],    # Muestra 2  \n",
    "           [7.0, 3.2],    # Muestra 3\n",
    "           [6.4, 3.2],    # Muestra 4\n",
    "           [6.3, 3.3],    # Muestra 5\n",
    "           [5.8, 2.7]]    # Muestra 6\n",
    "\n",
    "y_train = [0, 0, 1, 1, 2, 2]  # 0=Setosa, 1=Versicolor, 2=Virginica\n",
    "```\n",
    "\n",
    "### Paso 1: Detección de Clases\n",
    "OVA detecta automáticamente: `clases_unicas = [0, 1, 2]` → K = 3\n",
    "\n",
    "### Paso 2: Creación de Clasificadores Binarios\n",
    "\n",
    "**Clasificador 1 - \"Setosa vs Resto\":**\n",
    "```\n",
    "y_binario_1 = [1, 1, 0, 0, 0, 0]  # 1 si es Setosa, 0 si no\n",
    "# Entrena: LogisticRegression(X_train, y_binario_1)\n",
    "```\n",
    "\n",
    "**Clasificador 2 - \"Versicolor vs Resto\":**  \n",
    "```\n",
    "y_binario_2 = [0, 0, 1, 1, 0, 0]  # 1 si es Versicolor, 0 si no\n",
    "# Entrena: LogisticRegression(X_train, y_binario_2)\n",
    "```\n",
    "\n",
    "**Clasificador 3 - \"Virginica vs Resto\":**\n",
    "```\n",
    "y_binario_3 = [0, 0, 0, 0, 1, 1]  # 1 si es Virginica, 0 si no  \n",
    "# Entrena: LogisticRegression(X_train, y_binario_3)\n",
    "```\n",
    "\n",
    "### Paso 3: Predicción en Nuevas Muestras\n",
    "\n",
    "Para una nueva muestra `X_test = [6.1, 2.9]`:\n",
    "\n",
    "1. **Clasificador 1**: \"¿Es Setosa?\" → Score = 0.1 (baja confianza)\n",
    "2. **Clasificador 2**: \"¿Es Versicolor?\" → Score = 0.3 (media confianza)  \n",
    "3. **Clasificador 3**: \"¿Es Virginica?\" → Score = 0.7 (alta confianza)\n",
    "\n",
    "**Resultado**: `argmax([0.1, 0.3, 0.7]) = 2` → **Predicción: Virginica**\n",
    "\n",
    "### Ventaja Clave de OVA\n",
    "- ✅ **Simple**: Solo necesita 3 clasificadores binarios (vs 3 clasificadores en OvO sería 3×2/2 = 3, pero para K=10 sería OVA:10 vs OvO:45)\n",
    "- ✅ **Interpretable**: Cada clasificador tiene un propósito claro\n",
    "- ✅ **Escalable**: Tiempo de entrenamiento = O(K × tiempo_clasificador_binario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19515dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-vs-One (OvO) wrapper delegando en sklearn OneVsOneClassifier\n",
    "class OVOclasificador(BaseEstimator):\n",
    "\n",
    "    def __init__(self, C=1.0, solver='lbfgs', max_iter=1000):\n",
    "        # Hiperparámetros para los clasificadores binarios\n",
    "        self.C = C\n",
    "        self.solver = solver\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        # self.modelos: diccionario {(clase_a, clase_b): estimator}\n",
    "        # Se llena en Fit a partir de self.clf.estimators_\n",
    "        self.modelos = {}\n",
    "\n",
    "        # self.clases_: np.array de etiquetas únicas; antes None, después np.unique(y)\n",
    "        self.clases_ = None\n",
    "\n",
    "        # self.clf: instancia OneVsOneClassifier después de Fit\n",
    "        self.clf = None\n",
    "\n",
    "    def Fit(self, X, y):\n",
    "        # X: (n_samples, n_features), y: (n_samples,)\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        # Construir y entrenar OneVsOneClassifier\n",
    "        base = LogisticRegression(C=self.C, solver=self.solver, max_iter=self.max_iter)\n",
    "        self.clf = OneVsOneClassifier(base)\n",
    "        self.clf.fit(X, y)\n",
    "\n",
    "        # Guardar clases en el orden que sklearn usa internamente\n",
    "        self.clases_ = getattr(self.clf, 'classes_', None)\n",
    "\n",
    "        # estimators_ lista: contiene K*(K-1)/2 estimadores\n",
    "        estimators = getattr(self.clf, 'estimators_', None)\n",
    "        if estimators is not None and self.clases_ is not None:\n",
    "            # Asociar cada estimator con el par de clases correspondiente\n",
    "            # combinations(self.clases_, 2) genera pares en el mismo orden esperado\n",
    "            for (a, b), est in zip(list(combinations(self.clases_, 2)), estimators):\n",
    "                self.modelos[(a, b)] = est\n",
    "        else:\n",
    "            self.modelos = {}\n",
    "\n",
    "        return self\n",
    "\n",
    "    def Predict(self, X):\n",
    "        # X: (m, n_features)\n",
    "        X = np.asarray(X)\n",
    "        if self.clf is None:\n",
    "            raise RuntimeError(\"El clasificador no está entrenado. Ejecutar Fit primero.\")\n",
    "\n",
    "        # OneVsOneClassifier realiza la votación internamente y devuelve etiquetas\n",
    "        return self.clf.predict(X)\n",
    "\n",
    "# Ejemplo: para K=3 (clases {0,1,2}) se crean 3 clasificadores: (0,1),(0,2),(1,2)\n",
    "# estimators_ tendrá 3 elementos; self.modelos contendrá claves (0,1),(0,2),(1,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c47ec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de regresión logística multinomial (softmax) con descenso por gradiente\n",
    "class SoftmaxRegression(BaseEstimator):\n",
    "    \"\"\"Softmax implementado manualmente.\n",
    "\n",
    "    Parámetros y ejemplos:\n",
    "    - C: (opcional) si se proporciona, reg = 1/C (float). Ejemplo: C=1.0 -> reg=1.0\n",
    "    - lr: learning rate (float). Recomendado: 0.01 - 1.0 según problema. Valor por defecto 0.1.\n",
    "    - max_iter: iteraciones máximas (int), ejemplo 1000 o 2000.\n",
    "    - tol: tolerancia para convergencia en norma de cambio de W, ejemplo 1e-5.\n",
    "    - reg: regularización L2 directa (si se prefiere pasar explicitamente). Si reg no es None usa ese valor.\n",
    "\n",
    "    Shapes importantes:\n",
    "    - X: (n, d)\n",
    "    - Xb (con bias): (n, d+1)\n",
    "    - W: (d+1, K) donde K = número de clases\n",
    "    - probs: (n, K)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, C=None, lr=0.1, max_iter=1000, tol=1e-5, reg=None, verbose=False, **kwargs):\n",
    "        # Determinar regularización: si reg no se pasa, tomar 1/C si C fue provisto\n",
    "        if reg is None:\n",
    "            if C is None:\n",
    "                # Valor por defecto pequeño para evitar overfitting sin C\n",
    "                self.reg = 1e-3\n",
    "            else:\n",
    "                self.reg = 1.0 / C\n",
    "        else:\n",
    "            self.reg = reg\n",
    "\n",
    "        # Learning rate: controla tamaño de paso en descenso por gradiente\n",
    "        self.lr = lr\n",
    "        # Iteraciones máximas\n",
    "        self.max_iter = max_iter\n",
    "        # Tolerancia de convergencia en norma de cambio de W\n",
    "        self.tol = tol\n",
    "        # Verbose para imprimir progreso\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Pesos W inicializados en Fit; antes None\n",
    "        self.W = None\n",
    "        # Clases detectadas en Fit; antes None\n",
    "        self.classes_ = None\n",
    "\n",
    "    def _one_hot(self, y_idx, K):\n",
    "        # y_idx: vector de índices en rango 0..K-1, shape (n,)\n",
    "        n = y_idx.shape[0]\n",
    "        Y = np.zeros((n, K), dtype=float)\n",
    "        Y[np.arange(n), y_idx] = 1.0\n",
    "        return Y\n",
    "\n",
    "    def Fit(self, X, y):\n",
    "        # Asegurar arrays numpy\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        # Detectar clases únicas y mapear a índices 0..K-1\n",
    "        self.classes_ = np.unique(y)\n",
    "        K = len(self.classes_)\n",
    "        class_to_idx = {c: i for i, c in enumerate(self.classes_)}\n",
    "        # y_idx contiene índices 0..K-1 correspondientemente\n",
    "        y_idx = np.vectorize(class_to_idx.get)(y)\n",
    "\n",
    "        # Añadir columna de unos para bias: Xb shape (n, d+1)\n",
    "        n, d = X.shape\n",
    "        Xb = np.hstack([np.ones((n, 1)), X])\n",
    "\n",
    "        # Inicializar W: ceros (d+1, K). Alternativa: pequeña aleatoriedad ayuda a romper simetrías\n",
    "        self.W = np.zeros((d + 1, K), dtype=float)\n",
    "\n",
    "        # Matriz one-hot de etiquetas Y shape (n, K)\n",
    "        Y = self._one_hot(y_idx, K)\n",
    "\n",
    "        # Descenso por gradiente\n",
    "        for it in range(self.max_iter):\n",
    "            # logits: Xb.dot(W) -> shape (n, K)\n",
    "            scores = Xb.dot(self.W)\n",
    "            # Estabilizar antes de softmax\n",
    "            scores -= scores.max(axis=1, keepdims=True)\n",
    "            exp_scores = np.exp(scores)\n",
    "            probs = exp_scores / exp_scores.sum(axis=1, keepdims=True)  # (n, K)\n",
    "\n",
    "            # Gradiente de la función de pérdida (cross-entropy + L2)\n",
    "            grad = - (Xb.T.dot(Y - probs)) / n  # shape (d+1, K)\n",
    "            # Regularización L2 aplicada solo a pesos (no bias): primera fila de W no penalizada\n",
    "            reg_term = self.reg * np.vstack([np.zeros((1, K)), self.W[1:, :]])\n",
    "            grad += reg_term\n",
    "\n",
    "            # Actualizar W\n",
    "            W_old = self.W.copy()\n",
    "            self.W -= self.lr * grad\n",
    "\n",
    "            # Norm of change: criterio de parada\n",
    "            diff = np.linalg.norm(self.W - W_old)\n",
    "            if self.verbose and (it % 100 == 0 or it == self.max_iter - 1):\n",
    "                # Loss aproximado: cross-entropy media + L2 (sin bias)\n",
    "                loss = -np.mean(np.sum(Y * np.log(probs + 1e-15), axis=1)) + 0.5 * self.reg * np.sum(self.W[1:, :] ** 2)\n",
    "                print(f\"it={it} loss={loss:.6f} ||dW||={diff:.6e}\")\n",
    "            if diff < self.tol:\n",
    "                # Convergencia alcanzada\n",
    "                break\n",
    "\n",
    "        return self\n",
    "\n",
    "    def Predict(self, X):\n",
    "        # Comprobar que W fue entrenada\n",
    "        if self.W is None:\n",
    "            raise RuntimeError(\"El clasificador no está entrenado. Ejecutar Fit primero.\")\n",
    "        X = np.asarray(X)\n",
    "        n = X.shape[0]\n",
    "        Xb = np.hstack([np.ones((n, 1)), X])\n",
    "\n",
    "        # Calcular probabilidades y devolver etiqueta con mayor probabilidad\n",
    "        scores = Xb.dot(self.W)\n",
    "        scores -= scores.max(axis=1, keepdims=True)\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / exp_scores.sum(axis=1, keepdims=True)  # (n, K)\n",
    "        idx = np.argmax(probs, axis=1)\n",
    "        return self.classes_[idx]\n",
    "\n",
    "# Ejemplo de parámetros recomendados:\n",
    "# - lr=0.1, reg=1e-3 para datasets pequeños.\n",
    "# - Aumentar max_iter a 2000 si no converge.\n",
    "# - Usar verbose=True para ver progreso cada 100 it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2a1efda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OvA: accuracy=0.8444 time=0.050s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        15\n",
      "  versicolor       0.79      0.73      0.76        15\n",
      "   virginica       0.75      0.80      0.77        15\n",
      "\n",
      "    accuracy                           0.84        45\n",
      "   macro avg       0.85      0.84      0.84        45\n",
      "weighted avg       0.85      0.84      0.84        45\n",
      "\n",
      "Confusion matrix:\n",
      " [[15  0  0]\n",
      " [ 0 11  4]\n",
      " [ 0  3 12]]\n",
      "------------------------------------------------------------\n",
      "OvO: accuracy=0.9111 time=0.008s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        15\n",
      "  versicolor       0.82      0.93      0.88        15\n",
      "   virginica       0.92      0.80      0.86        15\n",
      "\n",
      "    accuracy                           0.91        45\n",
      "   macro avg       0.92      0.91      0.91        45\n",
      "weighted avg       0.92      0.91      0.91        45\n",
      "\n",
      "Confusion matrix:\n",
      " [[15  0  0]\n",
      " [ 0 14  1]\n",
      " [ 0  3 12]]\n",
      "------------------------------------------------------------\n",
      "Softmax: accuracy=0.9111 time=0.078s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        15\n",
      "  versicolor       0.82      0.93      0.88        15\n",
      "   virginica       0.92      0.80      0.86        15\n",
      "\n",
      "    accuracy                           0.91        45\n",
      "   macro avg       0.92      0.91      0.91        45\n",
      "weighted avg       0.92      0.91      0.91        45\n",
      "\n",
      "Confusion matrix:\n",
      " [[15  0  0]\n",
      " [ 0 14  1]\n",
      " [ 0  3 12]]\n",
      "------------------------------------------------------------\n",
      "Resumen:\n",
      "OvA: acc=0.8444 time=0.050s\n",
      "OvO: acc=0.9111 time=0.008s\n",
      "Softmax: acc=0.9111 time=0.078s\n"
     ]
    }
   ],
   "source": [
    "# Evaluación comparativa en el dataset Iris con explicaciones detalladas\n",
    "# Carga del dataset: 'data' contiene X (data.data) y y (data.target)\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import time\n",
    "\n",
    "# data: Bunch con campos .data (X) y .target (y)\n",
    "data = load_iris()\n",
    "# X: array (150, 4), y: array (150,) con etiquetas {0,1,2}\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Separar entrenamiento/prueba manteniendo proporciones de clase con 'stratify'\n",
    "# X_train: (105,4), X_test: (45,4) cuando test_size=0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Escalado: ajustar scaler en X_train y aplicar al conjunto de prueba\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)  # media ~0, desviación ~1 (por columna)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "# Instancias de los modelos. Estos objetos estarán sin entrenar hasta llamar a Fit.\n",
    "ova = OVAclasificador()\n",
    "ovo = OVOclasificador()\n",
    "try:\n",
    "    # intentar con firma de la implementación propia (lr, reg, max_iter)\n",
    "    sm = SoftmaxRegression(lr=0.5, max_iter=2000, tol=1e-7, verbose=False, reg=1e-3)\n",
    "except TypeError:\n",
    "    # fallback por compatibilidad\n",
    "    sm = SoftmaxRegression(C=1.0, solver='lbfgs', max_iter=2000)\n",
    "\n",
    "# Lista de tuplas (nombre, objeto) para iterar y comparar\n",
    "models = [('OvA', ova), ('OvO', ovo), ('Softmax', sm)]\n",
    "\n",
    "# results: dict donde guardamos info por modelo\n",
    "# keys: nombre modelo\n",
    "# values: dict con 'accuracy' (float), 'time_s' (float), 'y_pred' (array predicciones)\n",
    "results = {}\n",
    "\n",
    "for name, model in models:\n",
    "    # t0 guarda el tiempo antes de entrenar y predecir\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Entrenar el modelo con X_train_s (forma (n_train, n_features))\n",
    "    # Después de Fit, cada objeto tendrá internamente coeficientes/estados entrenados\n",
    "    model.Fit(X_train_s, y_train)\n",
    "\n",
    "    # Predict sobre X_test_s -> devuelve array (n_test,) con etiquetas predichas\n",
    "    y_pred = model.Predict(X_test_s)\n",
    "\n",
    "    # t1 guarda el tiempo después de entrenamiento+predicción\n",
    "    t1 = time.time()\n",
    "\n",
    "    # accuracy: número de aciertos / n_test\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Guardar resultados para posterior inspección\n",
    "    results[name] = {'accuracy': acc, 'time_s': t1 - t0, 'y_pred': y_pred}\n",
    "\n",
    "    # Imprimir métricas detalladas para este modelo\n",
    "    print(f\"{name}: accuracy={acc:.4f} time={t1-t0:.3f}s\")\n",
    "    # classification_report muestra precision/recall/f1 por clase (usa labels originales)\n",
    "    print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
    "    # confusion_matrix: matriz (n_clases, n_clases) donde rows=true, cols=pred\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Resumen compacto: mostrar accuracy y tiempos guardados en results\n",
    "print(\"Resumen:\")\n",
    "for name, info in results.items():\n",
    "    print(f\"{name}: acc={info['accuracy']:.4f} time={info['time_s']:.3f}s\")\n",
    "\n",
    "# Notas de interpretación:\n",
    "# - Si un modelo tiene baja precisión para una clase, inspeccionar su confusion matrix\n",
    "# - Softmax (implementación propia) puede requerir ajuste de lr y reg para converger bien\n",
    "# - Los tiempos reportados incluyen tanto entrenamiento como predicción\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
